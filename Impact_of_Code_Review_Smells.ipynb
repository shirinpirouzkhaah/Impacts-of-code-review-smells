{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Impact of Code Review Smells.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1buy9M4On6VSEHeZ9cVGq0xsR9u5NAziM",
      "authorship_tag": "ABX9TyPoIGw8WT49tBY+yfQov+oW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shirinpirouzkhaah/Impacts-of-code-review-smells/blob/notebook-conversion/Impact_of_Code_Review_Smells.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09yDHpkPO8lU"
      },
      "source": [
        "# Analyzing the Impact of Code Review Smells\n",
        "\n",
        "Welcome to the analysis notebook for this paper.\n",
        "\n",
        "Let's get you up and running!\n",
        "\n",
        "First things first: There cannot be any analysis until the data files are there. Let's fetch the data files.\n",
        "\n",
        "This can be accomplished by **running the code in the next cell!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUXmI299S43n"
      },
      "source": [
        "# this cell for setting some initial configuration options\n",
        "WORKING_DIRECTORY = \"/content/drive/MyDrive/oss project data/code_review_data\"\n",
        "DATA_FILES_LIST = [\n",
        "                   \"libreoffice.zip\",\n",
        "                   \"eclipse.zip\",\n",
        "                   \"qt.zip\",\n",
        "                   \"wireshark.zip\"\n",
        "]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JptwgLcnKNoY"
      },
      "source": [
        "# this cell helps you extract the code review data in order to be able to use them\n",
        "import zipfile\n",
        "for data_file in DATA_FILES_LIST:\n",
        "  with zipfile.ZipFile(WORKING_DIRECTORY + '/' + data_file, \"r\") as zip_ref:\n",
        "    zip_ref.extractall('./')\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0_dUG0VNOaF"
      },
      "source": [
        "# this cell will help you import the file you want to analyze the data for\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from scipy.stats import norm \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# constants: general\n",
        "NUM_OF_SECONDS_IN_ONE_DAY = 86400\n",
        "LOG_MESSAGE_TRIGGER_INDEX = 1000\n",
        "\n",
        "# read json project code review data\n",
        "cr_data_raw_json = pd.read_json (r'/content/libreoffice_cr_data.json', lines=True) \n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68HFBKCWN2A4"
      },
      "source": [
        "# this cell normalizes the top level data imported from the json file\n",
        "cr_data_normalized_json = pd.json_normalize(cr_data_raw_json['data'])\n",
        "cr_data_normalized_json = cr_data_normalized_json[cr_data_normalized_json.status != 'NEW']\n",
        "cr_data_normalized_json = cr_data_normalized_json.reset_index()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-5g6Mm_OSbb",
        "outputId": "e335d7db-e4b3-4c54-a39c-5000d7c6d67a"
      },
      "source": [
        "# this cell normalizes the patchset and comments data that will be used in further analysis\n",
        "\n",
        "# comments\n",
        "cr_data_comments = []\n",
        "for index,row in cr_data_normalized_json.iterrows():\n",
        "    cr_data_comments.append(pd.json_normalize(row['comments']))\n",
        "    if index % LOG_MESSAGE_TRIGGER_INDEX == 0:\n",
        "        print(f'{index} comments objects normalized!')\n",
        "\n",
        "\n",
        "# patchsets\n",
        "cr_data_patchsets = []\n",
        "for index,row in cr_data_normalized_json.iterrows():\n",
        "    cr_data_patchsets.append(pd.json_normalize(row['patchSets']))\n",
        "    if index % LOG_MESSAGE_TRIGGER_INDEX == 0:\n",
        "        print(f'{index} patchsets objects normalized!')\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 comments objects normalized!\n",
            "1000 comments objects normalized!\n",
            "2000 comments objects normalized!\n",
            "3000 comments objects normalized!\n",
            "4000 comments objects normalized!\n",
            "5000 comments objects normalized!\n",
            "6000 comments objects normalized!\n",
            "7000 comments objects normalized!\n",
            "8000 comments objects normalized!\n",
            "9000 comments objects normalized!\n",
            "10000 comments objects normalized!\n",
            "11000 comments objects normalized!\n",
            "12000 comments objects normalized!\n",
            "13000 comments objects normalized!\n",
            "14000 comments objects normalized!\n",
            "15000 comments objects normalized!\n",
            "16000 comments objects normalized!\n",
            "17000 comments objects normalized!\n",
            "18000 comments objects normalized!\n",
            "19000 comments objects normalized!\n",
            "20000 comments objects normalized!\n",
            "21000 comments objects normalized!\n",
            "22000 comments objects normalized!\n",
            "23000 comments objects normalized!\n",
            "24000 comments objects normalized!\n",
            "25000 comments objects normalized!\n",
            "26000 comments objects normalized!\n",
            "27000 comments objects normalized!\n",
            "28000 comments objects normalized!\n",
            "29000 comments objects normalized!\n",
            "30000 comments objects normalized!\n",
            "31000 comments objects normalized!\n",
            "32000 comments objects normalized!\n",
            "33000 comments objects normalized!\n",
            "34000 comments objects normalized!\n",
            "35000 comments objects normalized!\n",
            "36000 comments objects normalized!\n",
            "37000 comments objects normalized!\n",
            "38000 comments objects normalized!\n",
            "39000 comments objects normalized!\n",
            "40000 comments objects normalized!\n",
            "41000 comments objects normalized!\n",
            "42000 comments objects normalized!\n",
            "43000 comments objects normalized!\n",
            "44000 comments objects normalized!\n",
            "45000 comments objects normalized!\n",
            "46000 comments objects normalized!\n",
            "47000 comments objects normalized!\n",
            "48000 comments objects normalized!\n",
            "49000 comments objects normalized!\n",
            "50000 comments objects normalized!\n",
            "51000 comments objects normalized!\n",
            "52000 comments objects normalized!\n",
            "53000 comments objects normalized!\n",
            "54000 comments objects normalized!\n",
            "55000 comments objects normalized!\n",
            "56000 comments objects normalized!\n",
            "57000 comments objects normalized!\n",
            "58000 comments objects normalized!\n",
            "0 patchsets objects normalized!\n",
            "1000 patchsets objects normalized!\n",
            "2000 patchsets objects normalized!\n",
            "3000 patchsets objects normalized!\n",
            "4000 patchsets objects normalized!\n",
            "5000 patchsets objects normalized!\n",
            "6000 patchsets objects normalized!\n",
            "7000 patchsets objects normalized!\n",
            "8000 patchsets objects normalized!\n",
            "9000 patchsets objects normalized!\n",
            "10000 patchsets objects normalized!\n",
            "11000 patchsets objects normalized!\n",
            "12000 patchsets objects normalized!\n",
            "13000 patchsets objects normalized!\n",
            "14000 patchsets objects normalized!\n",
            "15000 patchsets objects normalized!\n",
            "16000 patchsets objects normalized!\n",
            "17000 patchsets objects normalized!\n",
            "18000 patchsets objects normalized!\n",
            "19000 patchsets objects normalized!\n",
            "20000 patchsets objects normalized!\n",
            "21000 patchsets objects normalized!\n",
            "22000 patchsets objects normalized!\n",
            "23000 patchsets objects normalized!\n",
            "24000 patchsets objects normalized!\n",
            "25000 patchsets objects normalized!\n",
            "26000 patchsets objects normalized!\n",
            "27000 patchsets objects normalized!\n",
            "28000 patchsets objects normalized!\n",
            "29000 patchsets objects normalized!\n",
            "30000 patchsets objects normalized!\n",
            "31000 patchsets objects normalized!\n",
            "32000 patchsets objects normalized!\n",
            "33000 patchsets objects normalized!\n",
            "34000 patchsets objects normalized!\n",
            "35000 patchsets objects normalized!\n",
            "36000 patchsets objects normalized!\n",
            "37000 patchsets objects normalized!\n",
            "38000 patchsets objects normalized!\n",
            "39000 patchsets objects normalized!\n",
            "40000 patchsets objects normalized!\n",
            "41000 patchsets objects normalized!\n",
            "42000 patchsets objects normalized!\n",
            "43000 patchsets objects normalized!\n",
            "44000 patchsets objects normalized!\n",
            "45000 patchsets objects normalized!\n",
            "46000 patchsets objects normalized!\n",
            "47000 patchsets objects normalized!\n",
            "48000 patchsets objects normalized!\n",
            "49000 patchsets objects normalized!\n",
            "50000 patchsets objects normalized!\n",
            "51000 patchsets objects normalized!\n",
            "52000 patchsets objects normalized!\n",
            "53000 patchsets objects normalized!\n",
            "54000 patchsets objects normalized!\n",
            "55000 patchsets objects normalized!\n",
            "56000 patchsets objects normalized!\n",
            "57000 patchsets objects normalized!\n",
            "58000 patchsets objects normalized!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QR4NNcbOGEm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rpoDEQVPimT"
      },
      "source": [
        "# this cell is for preprocessing some of the data so we can analyze it later\n",
        "from sys import breakpoint\n",
        "\n",
        "pr_completion_duration = []\n",
        "pr_num_of_iterations = []\n",
        "\n",
        "# for each merged pull request\n",
        "for index in range (len(cr_data_comments)): \n",
        "    num_of_iterations = 0\n",
        "    current_pr_comments = cr_data_comments[index]\n",
        "    last_comment = current_pr_comments.iloc[-1] # last operation of reviewing a PR\n",
        "    num_of_comments = len(current_pr_comments.index)-1 # number of operations in reviewing a PR\n",
        "    \n",
        "    # NaN check\n",
        "    if (last_comment[\"reviewer.name\"] == last_comment[\"reviewer.name\"]  ):\n",
        "        # Bot check and drop\n",
        "        if (last_comment[\"reviewer.name\"].find(\"Bot\") != -1): \n",
        "            current_pr_comments.drop(current_pr_comments.tail(1).index,inplace =True) \n",
        "    \n",
        "    for indx in range (len(current_pr_comments)):\n",
        "        # occurrence of 'Uploaded' indicates the start of another iteration\n",
        "        if (current_pr_comments.iloc[indx][\"message\"].find('Uploaded') != -1):\n",
        "            num_of_iterations = num_of_iterations + 1\n",
        "\n",
        "    # substract time stamp of last review operation from first review operation         \n",
        "    current_pr_time_difference = current_pr_comments.tail(1)[\"timestamp\"].iloc[0] - current_pr_comments.head(1)[\"timestamp\"].iloc[0]\n",
        "    breakpoint()\n",
        "    current_pr_time_difference = current_pr_time_difference / NUM_OF_SECONDS_IN_ONE_DAY# convert seconds to days \n",
        "    pr_completion_duration.append(current_pr_time_difference) # dataframe of time of completion of PR reviews \n",
        "    pr_num_of_iterations.append(num_of_iterations) # dataframe of iterations in each PR \n",
        "\n",
        "print(pr_num_of_iterations[1:5])\n",
        "print(pr_completion_duration[1:5])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AV5kP_IoUhgz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}